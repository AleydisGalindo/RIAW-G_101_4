{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import collections\n",
    "import csv\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from array import array\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-process a tweet\n",
    "def build_terms(line):\n",
    "\n",
    "    filtered_line = line.lower() ## Transform in lowercase\n",
    "    filtered_line = filtered_line.split() ## Tokenize the text to get a list of terms\n",
    "    filtered_line = [re.sub(r'[^\\w\\s]', '', word) for word in filtered_line] # Removing non-words and non-whitespaces\n",
    "    \n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_line = [word for word in filtered_line if word not in stop_words]  ## Eliminate the stopwords \n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    filtered_line = [stemmer.stem(word) for word in filtered_line] ## Perform stemming\n",
    "\n",
    "    return filtered_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emoticons\n",
    "def remove_emoticons(text):\n",
    "    # Define a pattern to find all the emoticons\n",
    "    emoticon_pattern = re.compile(\"[\" u\"\\U0001F600-\\U0001F64F\" u\"\\U0001F300-\\U0001F5FF\" \n",
    "                                  u\"\\U0001F680-\\U0001F6FF\" u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                                  u\"\\U00002500-\\U00002BEF\" u\"\\U00002702-\\U000027B0\" \n",
    "                                  u\"\\U000024C2-\\U0001F251\" u\"\\U0001f926-\\U0001f937\" \n",
    "                                  u\"\\U00010000-\\U0010ffff\" u\"\\u2640-\\u2642\" \n",
    "                                  u\"\\u2600-\\u2B55\" u\"\\u200d\" \n",
    "                                  u\"\\u23cf\" u\"\\u23e9\" \n",
    "                                  u\"\\u231a\" u\"\\ufe0f\" \n",
    "                                  u\"\\u3030\" \"]+\", re.UNICODE)\n",
    "\n",
    "    # Replace emoticons with an empty string\n",
    "    text_without_emoticons = emoticon_pattern.sub('', text)\n",
    "\n",
    "    return str(text_without_emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to renove links\n",
    "def remove_links(text):\n",
    "    # Define a pattern to match URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    # Replace URLs with an empty string\n",
    "    text_without_links = url_pattern.sub('', text)\n",
    "\n",
    "    return str(text_without_links)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESS OF THE DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "\n",
    "with open('IRWA_data_2023/Rus_Ukr_war_data.json', 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "lines = [l.strip().replace(' +', ' ') for l in lines]\n",
    "\n",
    "tweet_information = {}\n",
    "for line in lines:\n",
    "\n",
    "        tweet_data = json.loads(line)\n",
    "\n",
    "        # Clean the text\n",
    "        tweet_text = tweet_data['full_text']\n",
    "        tweet_text = remove_emoticons(tweet_text)\n",
    "        tweet_text = remove_links(tweet_text)\n",
    "\n",
    "        # Extract relevant information\n",
    "        tweet_id = tweet_data['id_str']\n",
    "        tweet_date = tweet_data['created_at']\n",
    "        hashtags = [hashtag['text'] for hashtag in tweet_data['entities']['hashtags']]\n",
    "        likes = tweet_data['favorite_count']\n",
    "        retweets = tweet_data['retweet_count'] \n",
    "        twitter_username = tweet_data['user']['screen_name']\n",
    "        tweet_url = f\"https://twitter.com/{twitter_username}/status/{tweet_id}\"\n",
    "\n",
    "        processed_tweet = build_terms(tweet_text)\n",
    "\n",
    "        # Store all the tweet information\n",
    "        tweet_information[tweet_id] = {\n",
    "            'Tweet ID': tweet_id,\n",
    "            'Tweet Text': tweet_text,\n",
    "            'Processed Tweet': processed_tweet,\n",
    "            'Tweet Date': tweet_date,\n",
    "            'Hashtags': hashtags,\n",
    "            'Likes': likes,\n",
    "            'Retweets': retweets,\n",
    "            'Tweet_url': tweet_url\n",
    "        }\n",
    "\n",
    "# Map tweet IDs with document IDs for evaluation stage\n",
    "tweet_document_ids_map = {}\n",
    "tweet_document_ids_map1 = {}\n",
    "\n",
    "with open('IRWA_data_2023/Rus_Ukr_war_data_ids.csv', 'r') as map_file:\n",
    "    doc = csv.reader(map_file, delimiter='\\t')\n",
    "    for row in doc:\n",
    "        doc_id, tweet_id = row\n",
    "        tweet_document_ids_map[doc_id] = tweet_id\n",
    "        tweet_document_ids_map1[tweet_id] = doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to index creation\n",
    "def create_index_tfidf(lines, num_documents):\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  # term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  # document frequencies of terms in the corpus\n",
    "    url_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for line in lines:\n",
    "        tweet_data = json.loads(line)\n",
    "        tweet_id = tweet_data['id_str']\n",
    "        \n",
    "        doc_id = tweet_document_ids_map1[tweet_id]\n",
    "        terms = tweet_information[tweet_id]['Processed Tweet']\n",
    "        url_index[doc_id] = tweet_information[tweet_id]['Tweet_url']\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                current_page_index[term] = [doc_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        for term, posting in current_page_index.items():\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))\n",
    "            df[term] += 1\n",
    "\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, url_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rank documents\n",
    "def rank_documents_TF_IDF(terms, docs, index, idf, tf):\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query.\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        query_vector[termIndex] = query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n",
    "\n",
    "    # Cosine similarity\n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "    return result_docs, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rank documents with custom score and cosine similarity\n",
    "def rank_documents_with_custom_score(terms, docs, index):\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    query_terms_count = collections.Counter(terms)\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm  # Using TF for the query\n",
    "\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                # Compute the TF/len(doc) for the term in the document\n",
    "                tf_value = len([postings[0]])\n",
    "                t_id = tweet_document_ids_map[doc]\n",
    "                my_score = 0.25 * tweet_information[t_id]['Likes'] + 0.75 * tweet_information[t_id]['Retweets']\n",
    "                doc_vectors[doc][termIndex] = (tf_value / len(tweet_information[t_id]['Processed Tweet'])) + my_score\n",
    "\n",
    "    # Cosine similarity\n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "    return result_docs, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model with word embeddings\n",
    "nlp = spacy.load(\"en_core_web_md\")  # You can replace \"en_core_web_md\" with other available models\n",
    "\n",
    "# Function to calculate tweet representation using spaCy's word embeddings\n",
    "def calculate_tweet_representation(tweet):\n",
    "    tweet_vector = np.zeros(nlp.vocab.vectors.shape[1])\n",
    "    word_count = 0\n",
    "\n",
    "    for word in tweet:\n",
    "        if nlp.vocab.has_vector(word):\n",
    "            tweet_vector += nlp.vocab[word].vector\n",
    "            word_count += 1\n",
    "\n",
    "    if word_count > 0:\n",
    "        tweet_vector /= word_count\n",
    "\n",
    "    return tweet_vector\n",
    "\n",
    "# Function to rank documents using spaCy's word embeddings + cosine similarity\n",
    "def rank_documents_tweet2vec(terms, docs, index):\n",
    "    doc_vectors = defaultdict(lambda: np.zeros(nlp.vocab.vectors.shape[1]))\n",
    "    query_vector = np.zeros(nlp.vocab.vectors.shape[1])\n",
    "\n",
    "    # Calculate the query vector using spaCy's word embeddings\n",
    "    for term in terms:\n",
    "        if nlp.vocab.has_vector(term):\n",
    "            query_vector += nlp.vocab[term].vector\n",
    "\n",
    "    # Normalize the query vector\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    if query_norm > 0:\n",
    "        query_vector /= query_norm\n",
    "\n",
    "    for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "        if doc in docs:\n",
    "            # Calculate the tweet vector for the document using spaCy's word embeddings\n",
    "            t_id = tweet_document_ids_map[doc]\n",
    "            tweet_vector = calculate_tweet_representation(tweet_information[t_id]['Processed Tweet'])\n",
    "\n",
    "            # Normalize the tweet vector\n",
    "            tweet_norm = np.linalg.norm(tweet_vector)\n",
    "            if tweet_norm > 0:\n",
    "                tweet_vector /= tweet_norm\n",
    "\n",
    "            # Cosine similarity\n",
    "            cosine_similarity = np.dot(tweet_vector, query_vector)\n",
    "\n",
    "            doc_vectors[doc] = cosine_similarity\n",
    "\n",
    "    # Sort documents based on cosine similarity\n",
    "    doc_scores = sorted(doc_vectors.items(), key=lambda x: x[1], reverse=True)\n",
    "    result_docs = [doc[0] for doc in doc_scores]\n",
    "\n",
    "    return result_docs, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search docs for specific queries\n",
    "def search(query, index):\n",
    "    query = build_terms(query)\n",
    "    docs = []\n",
    "    try:\n",
    "        term_docs=[posting[0] for posting in index[query[0]]]\n",
    "        for d_id in term_docs:\n",
    "            t_id = tweet_document_ids_map[d_id]\n",
    "            intersection = set(tweet_information[t_id]['Processed Tweet']).intersection(set(query))\n",
    "            if set(query) == intersection:\n",
    "                docs.append(d_id)\n",
    "    except:\n",
    "        pass\n",
    "    return query, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 180.07 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf, url_index = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insert your query or END to stop (i.e.: presidents visiting Kyiv):\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president in Kyiv\n",
      "\n",
      "======================\n",
      "Top 20 results out of 5 for the searched query using TF-IDF:\n",
      "\n",
      "1.\u001b[1mDOC_ID\u001b[0m = doc_654 - \u001b[1mTWEET_ID\u001b[0m = 1575827125159940096 - \u001b[1mLIKES\u001b[0m = 2 - \u001b[1mRETWEETS\u001b[0m = 2 \n",
      "2.\u001b[1mDOC_ID\u001b[0m = doc_656 - \u001b[1mTWEET_ID\u001b[0m = 1575827101030064128 - \u001b[1mLIKES\u001b[0m = 22 - \u001b[1mRETWEETS\u001b[0m = 2 \n",
      "3.\u001b[1mDOC_ID\u001b[0m = doc_87 - \u001b[1mTWEET_ID\u001b[0m = 1575908651658641408 - \u001b[1mLIKES\u001b[0m = 7 - \u001b[1mRETWEETS\u001b[0m = 3 \n",
      "4.\u001b[1mDOC_ID\u001b[0m = doc_131 - \u001b[1mTWEET_ID\u001b[0m = 1575905170952982529 - \u001b[1mLIKES\u001b[0m = 12 - \u001b[1mRETWEETS\u001b[0m = 0 \n",
      "5.\u001b[1mDOC_ID\u001b[0m = doc_2099 - \u001b[1mTWEET_ID\u001b[0m = 1575566317415178240 - \u001b[1mLIKES\u001b[0m = 0 - \u001b[1mRETWEETS\u001b[0m = 0 \n",
      "\n",
      "======================\n",
      "Top 20 results out of 5 for the searched query using custom score:\n",
      "\n",
      "1.\u001b[1mDOC_ID\u001b[0m = doc_656 - \u001b[1mTWEET_ID\u001b[0m = 1575827101030064128 - \u001b[1mLIKES\u001b[0m = 22 - \u001b[1mRETWEETS\u001b[0m = 2 \n",
      "2.\u001b[1mDOC_ID\u001b[0m = doc_87 - \u001b[1mTWEET_ID\u001b[0m = 1575908651658641408 - \u001b[1mLIKES\u001b[0m = 7 - \u001b[1mRETWEETS\u001b[0m = 3 \n",
      "3.\u001b[1mDOC_ID\u001b[0m = doc_131 - \u001b[1mTWEET_ID\u001b[0m = 1575905170952982529 - \u001b[1mLIKES\u001b[0m = 12 - \u001b[1mRETWEETS\u001b[0m = 0 \n",
      "4.\u001b[1mDOC_ID\u001b[0m = doc_654 - \u001b[1mTWEET_ID\u001b[0m = 1575827125159940096 - \u001b[1mLIKES\u001b[0m = 2 - \u001b[1mRETWEETS\u001b[0m = 2 \n",
      "5.\u001b[1mDOC_ID\u001b[0m = doc_2099 - \u001b[1mTWEET_ID\u001b[0m = 1575566317415178240 - \u001b[1mLIKES\u001b[0m = 0 - \u001b[1mRETWEETS\u001b[0m = 0 \n",
      "\n",
      "======================\n",
      "Top 20 results out of 5 for the searched query using Tweet2Vec:\n",
      "\n",
      "1.\u001b[1mDOC_ID\u001b[0m = doc_87 - \u001b[1mTWEET_ID\u001b[0m = 1575908651658641408 - \u001b[1mLIKES\u001b[0m = 7 - \u001b[1mRETWEETS\u001b[0m = 3 \n",
      "2.\u001b[1mDOC_ID\u001b[0m = doc_131 - \u001b[1mTWEET_ID\u001b[0m = 1575905170952982529 - \u001b[1mLIKES\u001b[0m = 12 - \u001b[1mRETWEETS\u001b[0m = 0 \n",
      "3.\u001b[1mDOC_ID\u001b[0m = doc_654 - \u001b[1mTWEET_ID\u001b[0m = 1575827125159940096 - \u001b[1mLIKES\u001b[0m = 2 - \u001b[1mRETWEETS\u001b[0m = 2 \n",
      "4.\u001b[1mDOC_ID\u001b[0m = doc_656 - \u001b[1mTWEET_ID\u001b[0m = 1575827101030064128 - \u001b[1mLIKES\u001b[0m = 22 - \u001b[1mRETWEETS\u001b[0m = 2 \n",
      "5.\u001b[1mDOC_ID\u001b[0m = doc_2099 - \u001b[1mTWEET_ID\u001b[0m = 1575566317415178240 - \u001b[1mLIKES\u001b[0m = 0 - \u001b[1mRETWEETS\u001b[0m = 0 \n",
      "\n",
      "Insert your query or END to stop (i.e.: presidents visiting Kyiv):\n",
      "\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "q = True\n",
    "while q == True:\n",
    "    print(\"\\nInsert your query or END to stop (i.e.: presidents visiting Kyiv):\\n\")\n",
    "    query = input()\n",
    "    print(query)\n",
    "    if query == 'END':\n",
    "        break\n",
    "    query, docs = search(query, index)\n",
    "    if len(docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "    else:\n",
    "        ranked_docs, cosine_similarity = rank_documents_TF_IDF(query, docs, index, idf, tf)\n",
    "        ranked_docs_custom, cosine_similarity2 = rank_documents_with_custom_score(query, docs, index)\n",
    "        ranked_docs_tweet2vec, cosine_similarity3 = rank_documents_tweet2vec(query, docs, index)\n",
    "        top = 20\n",
    "        print(\"\\n======================\\nTop {} results out of {} for the searched query using TF-IDF:\\n\".format(top, len(ranked_docs)))\n",
    "        ctr = 1\n",
    "        for d_id in ranked_docs[:top]:\n",
    "            t_id = tweet_document_ids_map[d_id]\n",
    "            print(\"{}.\\033[1mDOC_ID\\033[0m = {} - \\033[1mTWEET_ID\\033[0m = {} - \\033[1mLIKES\\033[0m = {} - \\033[1mRETWEETS\\033[0m = {} \".format(ctr, d_id, t_id, tweet_information[t_id]['Likes'], tweet_information[t_id]['Retweets']))\n",
    "            ctr += 1\n",
    "            print(\"{}.\\033[1mDOC_ID\\033[0m = {} - \\033[1mTWEET_ID\\033[0m = {} - \\033[1mTWEET_DATE\\033[0m = {} - \\033[1mHASHTAGS\\033[0m = {} - \\033[1mLIKES\\033[0m = {} - \\033[1mRETWEETS\\033[0m = {} - \\033[1mTWEET_URL\\033[0m = {}\".format(ctr, d_id, t_id, tweet_information[t_id]['Tweet Date'], tweet_information[t_id]['Hashtags'], tweet_information[t_id]['Likes'], tweet_information[t_id]['Retweets'], url_index[d_id]))\n",
    "        print(\"\\n======================\\nTop {} results out of {} for the searched query using custom score:\\n\".format(top, len(ranked_docs)))\n",
    "        ctr = 1\n",
    "        for d_id in ranked_docs_custom[:top]:\n",
    "            t_id = tweet_document_ids_map[d_id]\n",
    "            print(\"{}.\\033[1mDOC_ID\\033[0m = {} - \\033[1mTWEET_ID\\033[0m = {} - \\033[1mLIKES\\033[0m = {} - \\033[1mRETWEETS\\033[0m = {} \".format(ctr, d_id, t_id, tweet_information[t_id]['Likes'], tweet_information[t_id]['Retweets']))\n",
    "            ctr += 1\n",
    "        print(\"\\n======================\\nTop {} results out of {} for the searched query using Tweet2Vec:\\n\".format(top, len(ranked_docs)))\n",
    "        ctr = 1\n",
    "        for d_id in ranked_docs_tweet2vec[:top]:\n",
    "            t_id = tweet_document_ids_map[d_id]\n",
    "            print(\"{}.\\033[1mDOC_ID\\033[0m = {} - \\033[1mTWEET_ID\\033[0m = {} - \\033[1mLIKES\\033[0m = {} - \\033[1mRETWEETS\\033[0m = {} \".format(ctr, d_id, t_id, tweet_information[t_id]['Likes'], tweet_information[t_id]['Retweets']))\n",
    "            ctr += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
